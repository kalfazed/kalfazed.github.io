<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>project-autonomous-driving</title>

    <!-- box icons -->
    <link href='https://unpkg.com/boxicons@2.1.4/css/boxicons.min.css' rel='stylesheet'>

    <!-- custom css -->
    <link rel="stylesheet" href="css/style.css">
</head>

<body>

    <!-- header design -->
    <header class="header">
        <a href="#" class="logo">Portfolio.</a>

        <div class='bx bx-menu' id="menu-icon"></div>

        <nav class="navbar">
            <a href="../index.html#home">Home</a>
            <a href="../index.html#about">About</a>
            <a href="../index.html#projects" class="active">Projects</a>
            <a href="../index.html#hobbies">Hobbies</a>
            <a href="../index.html#contact">Contact</a>
        </nav>
    </header>

    <!-- about section design -->
    <section class="about" id="about">
        <div class="about-img">
            <figure>
              <img src="../images/bevfusion-flow.png" alt="sdlfksldkf">
              <figcaption>Fig1. The model inference flow of bevfusion. The model takes 6 cameras 
                and LiDAR as input, and generate 3D detection boxes in BEV</figcaption>
            </figure>
            <figure>
              <img src="../images/bevfision-result.png" alt="">
              <figcaption>Fig2. Qualitative results of BEVFusion on 3D object detection and BEV map 
                segmentation. It accurately recognizes distant and small objects (top) and parses 
                crowded nighttime scenes (bottom).</figcaption>
            </figure>
        </div>

        <div class="about-content">
            <h2 class="heading">Building autonomous driving</h2>
            <h3>(Give me a place to stand and I will move the world - Archimedes)</h3>

            <p>Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. 
              Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. 
              However, the camera-to-LiDAR projection throws away the semantic density of camera features, 
              hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). 
              In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor 
              fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely 
              preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in 
              the view transformation with optimized BEV pooling, reducing latency by more than 40x. 
              BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. 
              It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher 
              mIoU on BEV map segmentation, with 1.9x lower computation cost.</p>
            
            <p>In the early stage of perception systems, people design separate deep models for each sensor and fus
              e information via post-processing approaches. This method is limited by the loss of intermediate inf
              ormation. Recently, people have designed LiDAR-camera fusion deep networks to better leverage inform
              ation from both modalities. Specifically, the majority of works can be summarized as follow: i) give
              n one or a few points of the LiDAR point cloud, LiDAR to world transformation matrix and the essenti
              al matrix (camera to world); ii) people transform the LiDAR points or proposals into camera world an
              d use them as queries, to select corresponding image features. This line of work constitutes the sta
              te-of-the-art methods of 3D perception.</p>

            <p>However, one underlying assumption that people overlooked i
              s, that as one needs to generate image queries from LiDAR points, the current LiDAR-camera fusion me
              thods intrinsically depend on the raw point cloud of the LiDAR sensor. In the realistic world, peopl
              e discover that if the LiDAR sensor input is missing, for example, LiDAR points reflection rate is l
              ow due to object texture, a system glitch of internal data transfer, or even the field of view of th
              e LiDAR sensor cannot reach 360 degrees due to hardware limitations, and current fusion methods fail
               to produce meaningful results. This fundamentally hinders the applicability of this line of work in
              the realistic autonomous driving system.</p>
        </div>
    </section>

    <section class="about" id="about">
        <div class="about-content">
            <p>Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. 
              Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. 
              However, the camera-to-LiDAR projection throws away the semantic density of camera features, 
              hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). 
              In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor 
              fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely 
              preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in 
              the view transformation with optimized BEV pooling, reducing latency by more than 40x. 
              BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. 
              It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher 
              mIoU on BEV map segmentation, with 1.9x lower computation cost.</p>
            
            <p>In the early stage of perception systems, people design separate deep models for each sensor and fus
              e information via post-processing approaches. This method is limited by the loss of intermediate inf
              ormation. Recently, people have designed LiDAR-camera fusion deep networks to better leverage inform
              ation from both modalities. Specifically, the majority of works can be summarized as follow: i) give
              n one or a few points of the LiDAR point cloud, LiDAR to world transformation matrix and the essenti
              al matrix (camera to world); ii) people transform the LiDAR points or proposals into camera world an
              d use them as queries, to select corresponding image features. This line of work constitutes the sta
              te-of-the-art methods of 3D perception.</p>

            <p>However, one underlying assumption that people overlooked i
              s, that as one needs to generate image queries from LiDAR points, the current LiDAR-camera fusion me
              thods intrinsically depend on the raw point cloud of the LiDAR sensor. In the realistic world, peopl
              e discover that if the LiDAR sensor input is missing, for example, LiDAR points reflection rate is l
              ow due to object texture, a system glitch of internal data transfer, or even the field of view of th
              e LiDAR sensor cannot reach 360 degrees due to hardware limitations, and current fusion methods fail
               to produce meaningful results. This fundamentally hinders the applicability of this line of work in
              the realistic autonomous driving system.</p>
        </div>
        <!-- <div class="about-img"> -->
        <!--     <figure> -->
        <!--       <img src="../images/bevfusion-flow.png" alt="sdlfksldkf"> -->
        <!--       <figcaption>Fig1. The model inference flow of bevfusion. The model takes 6 cameras  -->
        <!--         and LiDAR as input, and generate 3D detection boxes in BEV</figcaption> -->
        <!--     </figure> -->
        <!--     <figure> -->
        <!--       <img src="../images/bevfision-result.png" alt=""> -->
        <!--       <figcaption>Fig2. Qualitative results of BEVFusion on 3D object detection and BEV map  -->
        <!--         segmentation. It accurately recognizes distant and small objects (top) and parses  -->
        <!--         crowded nighttime scenes (bottom).</figcaption> -->
        <!--     </figure> -->
        <!-- </div> -->
    </section>

    <!-- footer design -->
    <footer class="footer">
        <div class="footer-text">
            <p>Copyright &copy; 2023 by kalfazed | All Rights Reserved.</p>
        </div>

        <div class="footer-iconTop">
            <a href="#about"><i class='bx bx-up-arrow-alt'></i></a>
        </div>
    </footer>


    <!-- scroll reveal -->
    <script src="https://unpkg.com/scrollreveal"></script>

    <!-- typed js -->
    <script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12"></script>

    <!-- custom js -->
    <script src="../js/script.js"></script>
</body>

</html>
